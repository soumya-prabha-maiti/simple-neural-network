{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network from scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMadL_MkfgV_"
      },
      "source": [
        "#Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXoFOzYle_Ud"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4AWY7RXfyxQ"
      },
      "source": [
        "#Initializing parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFw3jrl5f41k"
      },
      "source": [
        "def initialize_parameters(layer_dims_incl_input):\n",
        "    \"\"\"\n",
        "    Initializes the weights(random initialization) and biases(initialization with zeros)\n",
        "\n",
        "    Arguments :\n",
        "    layer_dims_incl_input -- python list containing the dimensions of each layer(including input layer), i.e. , no of units in each layer in our network\n",
        "    \n",
        "    Returns :\n",
        "    parameters -- python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\"\n",
        "    Wl -- weight matrix , a numpy array of shape (layer_dims_incl_input[l], layer_dims_incl_input[l-1])\n",
        "    bl -- bias vector , a numpy array of shape (layer_dims_incl_input[l], 1)\n",
        "    \"\"\"\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L_i = len(layer_dims_incl_input) # number of layers in the network including input\n",
        "\n",
        "    for l in range(1, L_i):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims_incl_input[l], layer_dims_incl_input[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims_incl_input[l],1))\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipIU8kWc82B_"
      },
      "source": [
        "#Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPoS3jFJ84Zp"
      },
      "source": [
        "def sigmoid(Z):\n",
        "    return (1/(1+np.exp(-Z)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTh64QWW9FLU"
      },
      "source": [
        "def relu(Z):\n",
        "    return np.maximum(0,Z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HhpA-1rL_F2"
      },
      "source": [
        "#Forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi3GLtfgL-nG"
      },
      "source": [
        "def forward_1_layer(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for a single layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data) with shape =(size of previous layer, number of examples)\n",
        "    W -- weights matrix for current layer, numpy array of shape =(size of current layer, size of previous layer)\n",
        "    b -- bias vector for current layer, numpy array of shape =(size of the current layer, 1)\n",
        "    activation -- the activation function used in current layer , stored as a text string : \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns\n",
        "    A -- activation of current layer, obtained as the output of the activation function for the current layer\n",
        "    cache -- a python dictionary containing A_prev,W,b and Z;stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    Z=np.dot(W,A_prev)+b\n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "        A = sigmoid(Z)\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "        A = relu(Z)\n",
        "        \n",
        "    cache = {\n",
        "        \"A_prev\" : A_prev ,\n",
        "        \"W\" : W ,\n",
        "        \"b\" : b ,\n",
        "        \"Z\" : Z\n",
        "    }\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJQ8xeu_MwSz"
      },
      "source": [
        "def forward_L_layers(X, parameters,activations):\n",
        "    \"\"\"\n",
        "    Implements forward propagation for all the layers of the neural network \n",
        "    \n",
        "    Arguments:\n",
        "    X -- input features(pixel values in our case) for all records(all images), numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters()\n",
        "    activations -- python list of size no_of_layers_including_input containing activations for all the layers; activations[l]='activation function of layer l' ;activations[0] is non-existent\n",
        "\n",
        "    Returns:\n",
        "    AL -- activation value from the output layer(last layer of the network)\n",
        "    caches -- pyhton list of size no_of_layers_including_input containing every cache (every cache returned by forward_1_layer() function); caches[l]=cache for layer l; cache[0] is non-existent\n",
        "    \"\"\"\n",
        "    caches = [{}]\n",
        "    \n",
        "    A = X\n",
        "    L = len(parameters) // 2 # number of layers in the neural network excluding input layer\n",
        "    \n",
        "    # The for loop goes from 1 to L because layer 0 is the input\n",
        "    for l in range(1, L+1):\n",
        "        A_prev = A \n",
        "\n",
        "        A, cache = forward_1_layer(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], activations[l])\n",
        "        caches.append(cache)\n",
        "    \n",
        "    # At the end of for loop , A becomes AL, activation of last layer \n",
        "    return A, caches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcPbWVg9ScKU"
      },
      "source": [
        "#Calculating cost\n",
        "\n",
        "Computes the binary cross-entropy cost $J$, using the following formula: $$J=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze3KFIxpSVjV"
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implements the binary cross entropy cost function.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- activation of the last layer of the network which denotes the probability vector corresponding to our label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if the digit is zero, 1 if non-zero), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- binary cross-entropy cost\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]\n",
        "    cost =(-1/m)*(np.dot(Y,(np.log(AL)).T) + np.dot(1-Y,(np.log(1-AL)).T))\n",
        "  \n",
        "    cost = np.squeeze(cost) # To make sure cost is a number (int or float); e.g. this turns [[10]] into 10\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9ZpHFjuAxAV"
      },
      "source": [
        "#Derivatives of activation funtions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBvzaLaFA1Ek"
      },
      "source": [
        "def sigmoid_derivative(Z):\n",
        "    return sigmoid(Z)*(1-sigmoid(Z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYn35cimBMbD"
      },
      "source": [
        "def relu_derivative(Z):\n",
        "    return (Z>0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLEJ-PaDTnrw"
      },
      "source": [
        "#Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70_4Qvy61uEz"
      },
      "source": [
        "Initialising backpropagation(for binary cross entropy loss):\n",
        "\n",
        "$$dA^{[L]} = \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}=- (np.divide(Y, A^{[L]}) - np.divide(1 - Y, 1 - A^{[L]}))$$\n",
        "\n",
        "General formula:\n",
        "$$dZ^{[l]} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}} = dA^{[l]} * g'(Z^{[l]})$$\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqjQ7Nq6UAYX"
      },
      "source": [
        "def backward_1_layer(dA, cache, activation):\n",
        "    \n",
        "    A_prev=cache[\"A_prev\"]\n",
        "    W=cache[\"W\"]\n",
        "    b=cache[\"b\"]  \n",
        "    Z=cache[\"Z\"]\n",
        "    m=A_prev.shape[1]\n",
        "\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        dZ = dA * relu_derivative(Z)\n",
        "  \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ =  dA * sigmoid_derivative(Z)\n",
        "\n",
        "\n",
        "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
        "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
        "    dA_prev = np.dot(W.T,dZ)    \n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRu6QMj6XUj4"
      },
      "source": [
        "def backward_L_layers(AL, Y, caches , activations):\n",
        "    \n",
        "    grads = {}\n",
        "    L = len(caches)-1 # the number of layers excluding input\n",
        "    m = AL.shape[1]\n",
        "    \n",
        "    # Initializing the backpropagation(for cross entropy loss)\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    dA=dAL\n",
        "    \n",
        "    \n",
        "    # Loop from l=L to l=1\n",
        "    for l in reversed(range(1,L+1)):\n",
        "        current_cache =  caches[l]\n",
        "        dA_prev, dW, db = backward_1_layer(dA, current_cache, activations[l])\n",
        "        grads[\"dA\" + str(l-1)] = dA_prev\n",
        "        grads[\"dW\" + str(l)] = dW\n",
        "        grads[\"db\" + str(l)] = db\n",
        "        dA=dA_prev\n",
        "       \n",
        "\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSfvflCD4cde"
      },
      "source": [
        "#Update parameters\n",
        "Update the parameters using gradient descent following the update rule\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$\n",
        "\n",
        "where $\\alpha$ is the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbKetwNL4M4U"
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \n",
        "    parameters = parameters.copy()\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Updating all parameters using for loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*grads[\"db\" + str(l+1)]\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGw1mngG5r6L"
      },
      "source": [
        "#Combining all the funtions to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1OS0iTB50iB"
      },
      "source": [
        "def neural_network_train(X, Y, layer_dims_incl_input,activations, learning_rate = 0.01, num_iterations = 3000, print_cost=False):\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    costs = []                         # list to keep track of cost\n",
        "    \n",
        "    # Parameters initialization\n",
        "    parameters = initialize_parameters(layer_dims_incl_input)\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation:\n",
        "        AL, caches = forward_L_layers(X, parameters,activations)\n",
        "  \n",
        "        \n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, Y)\n",
        "  \n",
        "    \n",
        "        # Backward propagation.\n",
        "        grads = backward_L_layers(AL, Y, caches , activations)    \n",
        "  \n",
        " \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "  \n",
        "\n",
        "        # Print the cost every 200 iterations\n",
        "        if print_cost and i % 200 == 0 or i == num_iterations - 1:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "\n",
        "\n",
        "        #Append the cost after every iteration\n",
        "        if i % 1 == 0 or i == num_iterations:\n",
        "            costs.append(np.squeeze(cost))\n",
        "    \n",
        "    return parameters, costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTNQNuz-F7Pv"
      },
      "source": [
        "#Predict function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aK19JOkF_UG"
      },
      "source": [
        "def neural_network_predict(X,Y,parameters,activations):\n",
        "    AL, caches = forward_L_layers(X, parameters,activations)\n",
        "    \n",
        "    Y_pred=np.array((AL>=0.5), dtype=int)\n",
        "\n",
        "    print(\"Accuracy in percentage is \"+str((100 - np.mean(np.abs(Y_pred - Y)))))\n",
        "\n",
        "    return Y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjy__HYU5pmN"
      },
      "source": [
        "#Loading and processing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro5qLmSQ5kOr"
      },
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0snStvTx2Lux"
      },
      "source": [
        "# Changing the labels to zero or non-zero\n",
        "Y_train=np.array((Y_train!=0), dtype=int)\n",
        "Y_test=np.array((Y_test!=0), dtype=int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDcmRfx92Sbb"
      },
      "source": [
        "plt.imshow(X_test[1])\n",
        "print(Y_test[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QolfTOj92PDW"
      },
      "source": [
        "X_train=(X_train/255)\n",
        "X_test=(X_test/255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA4Co8cZ2XeG"
      },
      "source": [
        "X_train=X_train.reshape(X_train.shape[0],-1).T\n",
        "X_test=X_test.reshape(X_test.shape[0],-1).T\n",
        "Y_train=Y_train.reshape(Y_train.shape[0],-1).T\n",
        "Y_test=Y_test.reshape(Y_test.shape[0],-1).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMW1ZpLt2bSu"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape )\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u66NL8mj50_7"
      },
      "source": [
        "#Training and Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TyM0Umj6NqW"
      },
      "source": [
        "layer_dims_incl_input=[X_train.shape[0],16,1]\n",
        "activations=['NULL','relu','sigmoid']\n",
        "parameters, costs = neural_network_train(X_train, Y_train, layer_dims_incl_input, activations, learning_rate = 0.01, num_iterations = 200, print_cost = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHKAjc0-5mRF"
      },
      "source": [
        "plt.plot(costs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MtYJCXZFEKK"
      },
      "source": [
        "predictions_test = neural_network_predict(X_test, Y_test, parameters,activations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ6VyEYu9P6S"
      },
      "source": [
        "X_test_pic=X_test.T.reshape(-1,28,28)\n",
        "\n",
        "plt.imshow(X_test_pic[0])\n",
        "print(predictions_test[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07kzwNxcPBdk"
      },
      "source": [
        "count=0\n",
        "for i in range(Y_test.shape[1]):\n",
        "  if(predictions_test[0][i]!=Y_test[0][i]):\n",
        "    count+=1\n",
        "\n",
        "print(\"The no of incorrect predictions is \"+str(count))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}